{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OussamaHaff/machine-learning-upskilling/blob/main/02-llms-from-scratch/03-data-sampling-with-sliding-window/data_sampling_with_sliding__window.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic input-target pairs generation"
      ],
      "metadata": {
        "id": "cfDSHudj8wV1"
      },
      "id": "cfDSHudj8wV1"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "\n",
        "tokeniser = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2ryKmKs24D6",
        "outputId": "cba8824c-056f-4804-edff-a74201d97024"
      },
      "id": "m2ryKmKs24D6",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-02-15T13:40:39.006428Z",
          "start_time": "2025-02-15T13:40:39.001369Z"
        },
        "id": "initial_id"
      },
      "source": [
        "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as data_file:\n",
        "  raw_text = data_file.read()\n"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokeniser.encode(raw_text)\n",
        "print(len(encoded_text))\n",
        "print(encoded_text[:52])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNAdmI7_3VLQ",
        "outputId": "662fd21e-c990-448c-c2d5-c18ec8dc1399"
      },
      "id": "LNAdmI7_3VLQ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove first 50 elements\n",
        "\n",
        "encoded_sample = encoded_text[50:]\n",
        "print(encoded_sample[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbbRXMHE39TZ",
        "outputId": "9fec6b2a-21bb-4394-c894-236aa31d3224"
      },
      "id": "sbbRXMHE39TZ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the input-target pairs\n",
        "### Using Vanilla Python"
      ],
      "metadata": {
        "id": "upFPy0kL41WS"
      },
      "id": "upFPy0kL41WS"
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "input_x = encoded_sample[:context_size]\n",
        "target_y = encoded_sample[1:context_size+1]\n",
        "\n",
        "print(input_x)\n",
        "print(f\"     {target_y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxtDF4bx4_jZ",
        "outputId": "d23f9792-b27e-46be-b44b-6eba61c549b7"
      },
      "id": "YxtDF4bx4_jZ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290, 4920, 2241, 287]\n",
            "     [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = encoded_sample[:i]\n",
        "  desired = encoded_sample[i]\n",
        "  print(context, \"----->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUZri33x63M4",
        "outputId": "f54189e4-202a-448e-cd71-b1d76928cb63"
      },
      "id": "jUZri33x63M4",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] -----> 4920\n",
            "[290, 4920] -----> 2241\n",
            "[290, 4920, 2241] -----> 287\n",
            "[290, 4920, 2241, 287] -----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = encoded_sample[:i]\n",
        "  desired = encoded_sample[i]\n",
        "  print(tokeniser.decode(context), \"----->\", tokeniser.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peD-QFCS8RwP",
        "outputId": "430888af-7262-42e0-e0f6-f0a96336de17"
      },
      "id": "peD-QFCS8RwP",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ----->  established\n",
            " and established ----->  himself\n",
            " and established himself ----->  in\n",
            " and established himself in ----->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using PyTorch's Tensors & DataLoader"
      ],
      "metadata": {
        "id": "zBDGFFV8v77j"
      },
      "id": "zBDGFFV8v77j"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  \"\"\"Contructs a PyTorch Dataset with \"\"\"\n",
        "  def __init__(self, text, tokeniser, sliding_window_max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    tokenised_text = tokeniser.encode(text)\n",
        "\n",
        "    for i in range(0, len(tokenised_text) - sliding_window_max_length, stride):\n",
        "      input_chunk = tokenised_text[i: i + sliding_window_max_length]\n",
        "      target_chunk = tokenised_text[i + 1 : i + sliding_window_max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, id):\n",
        "    return self.input_ids[id], self.target_ids[id]"
      ],
      "metadata": {
        "id": "Z7O2xgrEweQi"
      },
      "id": "Z7O2xgrEweQi",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(text, batch_size=4, sliding_window_max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "  tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(text, tokeniser, sliding_window_max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "QxlQ1WQpzFnQ"
      },
      "id": "QxlQ1WQpzFnQ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as dataset_file:\n",
        "  raw_text = dataset_file.read()\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, sliding_window_max_length=4, stride=2, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "id": "mitSor0M1oVu",
        "outputId": "42c5d20f-b2a6-4be3-9969-2ae6d45cfb02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mitSor0M1oVu",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}