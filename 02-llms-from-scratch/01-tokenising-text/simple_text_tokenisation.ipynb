{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OussamaHaff/machine-learning-upskilling/blob/main/02-llms-from-scratch/01-tokenising-text/simple_text_tokenisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stats about text to tokenise"
      ],
      "metadata": {
        "id": "7k0jVYiCxTcg"
      },
      "id": "7k0jVYiCxTcg"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "32106992-d7c1-4e01-afd0-32f078bd4a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20479\n"
          ]
        }
      ],
      "source": [
        "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as book_file:\n",
        "    raw_text = book_file.read()\n",
        "\n",
        "print(\"Total number of characters:\", len(raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 100 characters:\\n\", raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxDZONeHx2Rm",
        "outputId": "856a3b3f-5f33-43e3-ad30-29e86fb4ce58"
      },
      "id": "PxDZONeHx2Rm",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 100 characters:\n",
            " I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Regex\n",
        "\n",
        "\n",
        "\n",
        "*   Split test *on* whitespace character (`s` for space)"
      ],
      "metadata": {
        "id": "a6AkhhAi0BqB"
      },
      "id": "a6AkhhAi0BqB"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result_s_split_only = re.split(r'(\\s)', text)\n",
        "print(result_s_split_only)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw9ZzuLq0RdT",
        "outputId": "c330333b-3ffa-4c1e-bf9d-f3b26a34c76d"
      },
      "id": "Lw9ZzuLq0RdT",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Split on whitespace (s), comma (,) and period (.)"
      ],
      "metadata": {
        "id": "3Q2s9QlV1Mbs"
      },
      "id": "3Q2s9QlV1Mbs"
    },
    {
      "cell_type": "code",
      "source": [
        "result_punc_chars_split = re.split(r'([,.]|\\s)', text)\n",
        "print(result_punc_chars_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPer5cXq1meb",
        "outputId": "98ebb3df-aa23-43a9-ca47-201b8afa4762"
      },
      "id": "pPer5cXq1meb",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Strip the whitespace character"
      ],
      "metadata": {
        "id": "xO6O62-Z2Wc0"
      },
      "id": "xO6O62-Z2Wc0"
    },
    {
      "cell_type": "code",
      "source": [
        "result_strip_whitespace = [item for item in result_punc_chars_split if item.strip()]\n",
        "print(result_strip_whitespace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lVqDCxs2jAa",
        "outputId": "4cc97a46-02ad-480f-cb32-1e40c6f3802b"
      },
      "id": "5lVqDCxs2jAa",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*    Splitg all punctuation characters"
      ],
      "metadata": {
        "id": "AVhTbnax49Hw"
      },
      "id": "AVhTbnax49Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result_all_punc_chars_split = re.split(r'[.,:;?_!\"()\\']|--|\\s', text)\n",
        "print(result_all_punc_chars_split)\n",
        "\n",
        "result_all_punc_chars_split_cleaned = [item.strip() for item in result_all_punc_chars_split if item.strip()]\n",
        "print(\"Cleaned result:\\n\", result_all_punc_chars_split_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT_wQ_4_5FVa",
        "outputId": "5d9b880d-3272-4723-b235-0f7d5c7778f1"
      },
      "id": "bT_wQ_4_5FVa",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '', 'world', '', 'Is', 'this', '', 'a', 'test', '']\n",
            "Cleaned result:\n",
            " ['Hello', 'world', 'Is', 'this', 'a', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic tokenisation of sample data"
      ],
      "metadata": {
        "id": "JvWBhQMD7MVH"
      },
      "id": "JvWBhQMD7MVH"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'[.,:;?_!\"()\\']|--|\\s', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(\"Number of tokens:\", len(preprocessed))\n",
        "print(\"First 33 tokens:\\n\", preprocessed[:33])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcPqJZav7U7o",
        "outputId": "904abde5-bac4-49ad-8fdc-c6ae96944654"
      },
      "id": "kcPqJZav7U7o",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 3788\n",
            "First 33 tokens:\n",
            " ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', 'though', 'a', 'good', 'fellow', 'enough', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', 'in', 'the', 'height', 'of', 'his', 'glory', 'he']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic vocabulary of sample data"
      ],
      "metadata": {
        "id": "vdSPn6gRtQQq"
      },
      "id": "vdSPn6gRtQQq"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words = sorted(set(preprocessed))\n",
        "vocab_size = len(vocab_words)\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U1IXKRrtjsq",
        "outputId": "f81d6433-ca73-403e-abab-c583efee6643"
      },
      "id": "0U1IXKRrtjsq",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 1118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The actual vocabulary with IDs"
      ],
      "metadata": {
        "id": "-CRfWzXYukZd"
      },
      "id": "-CRfWzXYukZd"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = { token:integer for integer,token in enumerate(vocab_words) }\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp6-Zn5vuh-4",
        "outputId": "58af5550-941a-4e8c-bfed-857defeb9b90"
      },
      "id": "kp6-Zn5vuh-4",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A', 0)\n",
            "('Ah', 1)\n",
            "('Among', 2)\n",
            "('And', 3)\n",
            "('Are', 4)\n",
            "('Arrt', 5)\n",
            "('As', 6)\n",
            "('At', 7)\n",
            "('Be', 8)\n",
            "('Begin', 9)\n",
            "('Burlington', 10)\n",
            "('But', 11)\n",
            "('By', 12)\n",
            "('Carlo', 13)\n",
            "('Chicago', 14)\n",
            "('Claude', 15)\n",
            "('Come', 16)\n",
            "('Croft', 17)\n",
            "('Destroyed', 18)\n",
            "('Devonshire', 19)\n",
            "('Don', 20)\n",
            "('Dubarry', 21)\n",
            "('Emperors', 22)\n",
            "('Florence', 23)\n",
            "('For', 24)\n",
            "('Gallery', 25)\n",
            "('Gideon', 26)\n",
            "('Gisburn', 27)\n",
            "('Gisburns', 28)\n",
            "('Grafton', 29)\n",
            "('Greek', 30)\n",
            "('Grindle', 31)\n",
            "('Grindles', 32)\n",
            "('HAD', 33)\n",
            "('Had', 34)\n",
            "('Hang', 35)\n",
            "('Has', 36)\n",
            "('He', 37)\n",
            "('Her', 38)\n",
            "('Hermia', 39)\n",
            "('His', 40)\n",
            "('How', 41)\n",
            "('I', 42)\n",
            "('If', 43)\n",
            "('In', 44)\n",
            "('It', 45)\n",
            "('Jack', 46)\n",
            "('Jove', 47)\n",
            "('Just', 48)\n",
            "('Lord', 49)\n",
            "('Made', 50)\n",
            "('Miss', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Text Tokeniser"
      ],
      "metadata": {
        "id": "dHgs0wSYzmqc"
      },
      "id": "dHgs0wSYzmqc"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTextTokeniserV1:\n",
        "  def __init__(self, vocab):\n",
        "    \"\"\"\n",
        "    Saves the vocabulary of unique and sorted tokens,\n",
        "    then creates its inverse and saves it.\n",
        "    \"\"\"\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items() }\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\"\n",
        "    Preprocesses a text by splitting it on special chars,\n",
        "    then strip the text from all those chars.\n",
        "    then map the tokens to ids based on the vocab\n",
        "    \"\"\"\n",
        "    preprocessed = re.split(r'[.,:;!?()\"\\']|--|\\s', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Subtitues any occurence of a space + punctuation with the punctuation char\n",
        "    text = re.sub(r'\\s+([,.?!()\"\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Qn43FkM1zqlP"
      },
      "id": "Qn43FkM1zqlP",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying the tokeniser on the text\n",
        "\n",
        "#### Encode"
      ],
      "metadata": {
        "id": "YE31ToYP7PP7"
      },
      "id": "YE31ToYP7PP7"
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniser = SimpleTextTokeniserV1(vocab)\n",
        "text = \"\"\"\n",
        "  \"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
        "  \"\"\"\n",
        "ids = tokeniser.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xr3-_N87UrP",
        "outputId": "9edfa4cb-a6e9-4016-ca80-3497e3ce90bc"
      },
      "id": "9xr3-_N87UrP",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 838, 976, 590, 521, 734, 1114, 584, 56, 27, 839, 1096, 742, 781]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokeniser.decode(ids))"
      ],
      "metadata": {
        "id": "zTpl9Pnm8Jyo",
        "outputId": "da5d13a8-b5c9-42ff-9b30-dd5be40befd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zTpl9Pnm8Jyo",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It s the last he painted you know Mrs Gisburn said with pardonable pride\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}