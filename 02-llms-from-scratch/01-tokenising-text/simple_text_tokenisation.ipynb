{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OussamaHaff/machine-learning-upskilling/blob/main/02-llms-from-scratch/01-tokenising-text/simple_text_tokenisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stats about text to tokenise"
      ],
      "metadata": {
        "id": "7k0jVYiCxTcg"
      },
      "id": "7k0jVYiCxTcg"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "32106992-d7c1-4e01-afd0-32f078bd4a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20479\n"
          ]
        }
      ],
      "source": [
        "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as book_file:\n",
        "    raw_text = book_file.read()\n",
        "\n",
        "print(\"Total number of characters:\", len(raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 100 characters:\\n\", raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxDZONeHx2Rm",
        "outputId": "856a3b3f-5f33-43e3-ad30-29e86fb4ce58"
      },
      "id": "PxDZONeHx2Rm",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 100 characters:\n",
            " I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Regex\n",
        "\n",
        "\n",
        "\n",
        "*   Split test *on* whitespace character (`s` for space)"
      ],
      "metadata": {
        "id": "a6AkhhAi0BqB"
      },
      "id": "a6AkhhAi0BqB"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result_s_split_only = re.split(r'(\\s)', text)\n",
        "print(result_s_split_only)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw9ZzuLq0RdT",
        "outputId": "c330333b-3ffa-4c1e-bf9d-f3b26a34c76d"
      },
      "id": "Lw9ZzuLq0RdT",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Split on whitespace (s), comma (,) and period (.)"
      ],
      "metadata": {
        "id": "3Q2s9QlV1Mbs"
      },
      "id": "3Q2s9QlV1Mbs"
    },
    {
      "cell_type": "code",
      "source": [
        "result_punc_chars_split = re.split(r'([,.]|\\s)', text)\n",
        "print(result_punc_chars_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPer5cXq1meb",
        "outputId": "98ebb3df-aa23-43a9-ca47-201b8afa4762"
      },
      "id": "pPer5cXq1meb",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Strip the whitespace character"
      ],
      "metadata": {
        "id": "xO6O62-Z2Wc0"
      },
      "id": "xO6O62-Z2Wc0"
    },
    {
      "cell_type": "code",
      "source": [
        "result_strip_whitespace = [item for item in result_punc_chars_split if item.strip()]\n",
        "print(result_strip_whitespace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lVqDCxs2jAa",
        "outputId": "4cc97a46-02ad-480f-cb32-1e40c6f3802b"
      },
      "id": "5lVqDCxs2jAa",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*    Splitg all punctuation characters"
      ],
      "metadata": {
        "id": "AVhTbnax49Hw"
      },
      "id": "AVhTbnax49Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result_all_punc_chars_split = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
        "print(result_all_punc_chars_split)\n",
        "\n",
        "result_all_punc_chars_split_cleaned = [item.strip() for item in result_all_punc_chars_split if item.strip()]\n",
        "print(\"Cleaned result:\\n\", result_all_punc_chars_split_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT_wQ_4_5FVa",
        "outputId": "97c7dacc-aaab-4f0b-96c3-fffc77a29561"
      },
      "id": "bT_wQ_4_5FVa",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
            "Cleaned result:\n",
            " ['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic tokenisation of sample data"
      ],
      "metadata": {
        "id": "JvWBhQMD7MVH"
      },
      "id": "JvWBhQMD7MVH"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(\"Number of tokens:\", len(preprocessed))\n",
        "print(\"First 33 tokens:\\n\", preprocessed[:33])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcPqJZav7U7o",
        "outputId": "c961987f-622a-4a04-802d-1cafc483f838"
      },
      "id": "kcPqJZav7U7o",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 4690\n",
            "First 33 tokens:\n",
            " ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic vocabulary of sample data"
      ],
      "metadata": {
        "id": "vdSPn6gRtQQq"
      },
      "id": "vdSPn6gRtQQq"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words = sorted(set(preprocessed))\n",
        "vocab_size = len(vocab_words)\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U1IXKRrtjsq",
        "outputId": "8cf416cf-291a-41e9-9d88-18ed50c67063"
      },
      "id": "0U1IXKRrtjsq",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The actual vocabulary with IDs"
      ],
      "metadata": {
        "id": "-CRfWzXYukZd"
      },
      "id": "-CRfWzXYukZd"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = { token:integer for integer,token in enumerate(vocab_words) }\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp6-Zn5vuh-4",
        "outputId": "a775937f-6e77-4c0f-eb2d-50ac92928889"
      },
      "id": "kp6-Zn5vuh-4",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n",
            "('His', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Text Tokeniser"
      ],
      "metadata": {
        "id": "dHgs0wSYzmqc"
      },
      "id": "dHgs0wSYzmqc"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTextTokeniserV1:\n",
        "  def __init__(self, vocab):\n",
        "    \"\"\"\n",
        "    Saves the vocabulary of unique and sorted tokens,\n",
        "    then creates its inverse and saves it.\n",
        "    \"\"\"\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items() }\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\"\n",
        "    Preprocesses a text by splitting it on special chars,\n",
        "    then strip the text from all those chars.\n",
        "    then map the tokens to ids based on the vocab\n",
        "    \"\"\"\n",
        "    preprocessed = re.split(r'([.,:;!?()\"\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Subtitues any occurence of a space + punctuation with the punctuation char\n",
        "    text = re.sub(r'\\s+([,.?!()\"\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Qn43FkM1zqlP"
      },
      "id": "Qn43FkM1zqlP",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying the tokeniser on the text\n",
        "\n",
        "#### Encode"
      ],
      "metadata": {
        "id": "YE31ToYP7PP7"
      },
      "id": "YE31ToYP7PP7"
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniser = SimpleTextTokeniserV1(vocab)\n",
        "text = \"\"\"\n",
        "  \"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
        "  \"\"\"\n",
        "ids = tokeniser.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xr3-_N87UrP",
        "outputId": "4e857cf6-808e-4e94-9e0a-daaf0c9d399f"
      },
      "id": "9xr3-_N87UrP",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokeniser.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTpl9Pnm8Jyo",
        "outputId": "b2f61cff-14d0-4e96-f1ee-c6a09854b46d"
      },
      "id": "zTpl9Pnm8Jyo",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}