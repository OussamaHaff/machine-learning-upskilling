{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OussamaHaff/machine-learning-upskilling/blob/main/02-llms-from-scratch/01-tokenising-text/simple_text_tokenisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stats about text to tokenise"
      ],
      "metadata": {
        "id": "7k0jVYiCxTcg"
      },
      "id": "7k0jVYiCxTcg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "32106992-d7c1-4e01-afd0-32f078bd4a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20479\n"
          ]
        }
      ],
      "source": [
        "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as book_file:\n",
        "    raw_text = book_file.read()\n",
        "\n",
        "print(\"Total number of characters:\", len(raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 100 characters:\\n\", raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxDZONeHx2Rm",
        "outputId": "856a3b3f-5f33-43e3-ad30-29e86fb4ce58"
      },
      "id": "PxDZONeHx2Rm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 100 characters:\n",
            " I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Regex\n",
        "\n",
        "\n",
        "\n",
        "*   Split test *on* whitespace character (`s` for space)"
      ],
      "metadata": {
        "id": "a6AkhhAi0BqB"
      },
      "id": "a6AkhhAi0BqB"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result_s_split_only = re.split(r'(\\s)', text)\n",
        "print(result_s_split_only)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw9ZzuLq0RdT",
        "outputId": "c330333b-3ffa-4c1e-bf9d-f3b26a34c76d"
      },
      "id": "Lw9ZzuLq0RdT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Split on whitespace (s), comma (,) and period (.)"
      ],
      "metadata": {
        "id": "3Q2s9QlV1Mbs"
      },
      "id": "3Q2s9QlV1Mbs"
    },
    {
      "cell_type": "code",
      "source": [
        "result_punc_chars_split = re.split(r'([,.]|\\s)', text)\n",
        "print(result_punc_chars_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPer5cXq1meb",
        "outputId": "98ebb3df-aa23-43a9-ca47-201b8afa4762"
      },
      "id": "pPer5cXq1meb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Strip the whitespace character"
      ],
      "metadata": {
        "id": "xO6O62-Z2Wc0"
      },
      "id": "xO6O62-Z2Wc0"
    },
    {
      "cell_type": "code",
      "source": [
        "result_strip_whitespace = [item for item in result_punc_chars_split if item.strip()]\n",
        "print(result_strip_whitespace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lVqDCxs2jAa",
        "outputId": "4cc97a46-02ad-480f-cb32-1e40c6f3802b"
      },
      "id": "5lVqDCxs2jAa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*    Splitg all punctuation characters"
      ],
      "metadata": {
        "id": "AVhTbnax49Hw"
      },
      "id": "AVhTbnax49Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result_all_punc_chars_split = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
        "print(result_all_punc_chars_split)\n",
        "\n",
        "result_all_punc_chars_split_cleaned = [item.strip() for item in result_all_punc_chars_split if item.strip()]\n",
        "print(\"Cleaned result:\\n\", result_all_punc_chars_split_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT_wQ_4_5FVa",
        "outputId": "97c7dacc-aaab-4f0b-96c3-fffc77a29561"
      },
      "id": "bT_wQ_4_5FVa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n",
            "Cleaned result:\n",
            " ['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic tokenisation of sample data"
      ],
      "metadata": {
        "id": "JvWBhQMD7MVH"
      },
      "id": "JvWBhQMD7MVH"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(\"Number of tokens:\", len(preprocessed))\n",
        "print(\"First 33 tokens:\\n\", preprocessed[:33])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcPqJZav7U7o",
        "outputId": "c961987f-622a-4a04-802d-1cafc483f838"
      },
      "id": "kcPqJZav7U7o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 4690\n",
            "First 33 tokens:\n",
            " ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic vocabulary of sample data"
      ],
      "metadata": {
        "id": "vdSPn6gRtQQq"
      },
      "id": "vdSPn6gRtQQq"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words = sorted(set(preprocessed))\n",
        "vocab_size = len(vocab_words)\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U1IXKRrtjsq",
        "outputId": "8cf416cf-291a-41e9-9d88-18ed50c67063"
      },
      "id": "0U1IXKRrtjsq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The actual vocabulary with IDs"
      ],
      "metadata": {
        "id": "-CRfWzXYukZd"
      },
      "id": "-CRfWzXYukZd"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = { token:integer for integer,token in enumerate(vocab_words) }\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp6-Zn5vuh-4",
        "outputId": "a775937f-6e77-4c0f-eb2d-50ac92928889"
      },
      "id": "kp6-Zn5vuh-4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n",
            "('His', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Text Tokeniser"
      ],
      "metadata": {
        "id": "dHgs0wSYzmqc"
      },
      "id": "dHgs0wSYzmqc"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTextTokeniserV1:\n",
        "  def __init__(self, vocab):\n",
        "    \"\"\"\n",
        "    Saves the vocabulary of unique and sorted tokens,\n",
        "    then creates its inverse and saves it.\n",
        "    \"\"\"\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items() }\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\"\n",
        "    Preprocesses a text by splitting it on special chars,\n",
        "    then strip the text from all those chars.\n",
        "    then map the tokens to ids based on the vocab\n",
        "    \"\"\"\n",
        "    preprocessed = re.split(r'([.,:;!?()\"\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Subtitues any occurence of a space + punctuation with the punctuation char\n",
        "    text = re.sub(r'\\s+([,.?!()\"\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Qn43FkM1zqlP"
      },
      "id": "Qn43FkM1zqlP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying the tokeniser on the text\n",
        "\n",
        "#### Encode"
      ],
      "metadata": {
        "id": "YE31ToYP7PP7"
      },
      "id": "YE31ToYP7PP7"
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniser = SimpleTextTokeniserV1(vocab)\n",
        "text = \"\"\"\n",
        "  \"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
        "  \"\"\"\n",
        "ids = tokeniser.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xr3-_N87UrP",
        "outputId": "4e857cf6-808e-4e94-9e0a-daaf0c9d399f"
      },
      "id": "9xr3-_N87UrP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decode"
      ],
      "metadata": {
        "id": "-7UDV7jnDAuA"
      },
      "id": "-7UDV7jnDAuA"
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokeniser.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTpl9Pnm8Jyo",
        "outputId": "c7ffa9d8-c8b7-4fe8-cd86-2a8716cc5ccf"
      },
      "id": "zTpl9Pnm8Jyo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encode unknown tokens"
      ],
      "metadata": {
        "id": "uYWuLSdUDIc3"
      },
      "id": "uYWuLSdUDIc3"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "# This should print an error since 'Hello' does not exist in the original dataset\n",
        "print(tokeniser.encode(text))"
      ],
      "metadata": {
        "id": "Z-nE4aPyBPop",
        "outputId": "b0f7a575-0b82-4117-8d3f-5ef05b8d0181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "id": "Z-nE4aPyBPop",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-3c531e2dbcd6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokeniser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-8dd1eb95a54f>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([.,:;!?()\"\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-8dd1eb95a54f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([.,:;!?()\"\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Special tokens"
      ],
      "metadata": {
        "id": "S3cbs-N3DkRd"
      },
      "id": "S3cbs-N3DkRd"
    },
    {
      "cell_type": "code",
      "source": [
        "all_vocab_tokens = sorted(set(preprocessed))\n",
        "all_vocab_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = { token:integer for integer,token in enumerate(all_vocab_tokens) }\n",
        "\n",
        "print(\"Vocab length after adding 2 special chars:\", len(vocab.items()))"
      ],
      "metadata": {
        "id": "kgbjevvmEv9d",
        "outputId": "f220ae76-b9eb-4a65-8199-7a81691f185e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kgbjevvmEv9d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length after adding 2 special chars: 1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_vocab_tokens = enumerate(list(vocab.items())[-5:])\n",
        "for i,item in last_vocab_tokens:\n",
        "  print(item)\n"
      ],
      "metadata": {
        "id": "s_y9GJzWF7Lj",
        "outputId": "355273e7-3750-4610-b4e8-687b71c20190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s_y9GJzWF7Lj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTextTokeniserV2:\n",
        "  def __init__(self, vocab):\n",
        "    \"\"\"\n",
        "    Saves the vocabulary of unique and sorted tokens,\n",
        "    then creates its inverse and saves it.\n",
        "    \"\"\"\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items() }\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\"\n",
        "    Preprocesses a text by splitting it on special chars,\n",
        "    then strip the text from all those chars.\n",
        "    then map the tokens to ids based on the vocab\n",
        "    \"\"\"\n",
        "    preprocessed = re.split(r'([.,:;!?()\"\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Subtitues any occurence of a space + punctuation with the punctuation char\n",
        "    text = re.sub(r'\\s+([,.?!()\"\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "c8ymA84lD9e5"
      },
      "id": "c8ymA84lD9e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encode with special tokens"
      ],
      "metadata": {
        "id": "OBbNMVvzJAkZ"
      },
      "id": "OBbNMVvzJAkZ"
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terrace of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "ywhnxaStJGBO",
        "outputId": "b1129ad3-16d7-498b-c0d6-fcd3d302d172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ywhnxaStJGBO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terrace of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniserV2 = SimpleTextTokeniserV2(vocab)\n",
        "encoded_text = tokeniserV2.encode(text)\n",
        "print(encoded_text)"
      ],
      "metadata": {
        "id": "qE4Y0_RpJjnP",
        "outputId": "ffa4ac01-fb37-4b85-c052-fbe90fa50303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qE4Y0_RpJjnP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 983, 722, 988, 1131, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decode with special tokens"
      ],
      "metadata": {
        "id": "w2smnkCSJ_Ku"
      },
      "id": "w2smnkCSJ_Ku"
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniserV2.decode(encoded_text)"
      ],
      "metadata": {
        "id": "yJfm74zyKDIx",
        "outputId": "f4f1211e-c4f0-4912-ea8d-76e922f83868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "yJfm74zyKDIx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terrace of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise on Tokenisation"
      ],
      "metadata": {
        "id": "mZFf1GCfldwc"
      },
      "id": "mZFf1GCfldwc"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sentence = \"The joy of learning machine learning?!\"\n",
        "\n",
        "def tokenise_text_naive(text):\n",
        "  \"\"\"\n",
        "  Takes a string as input and returns a list of tokens\n",
        "  \"\"\"\n",
        "  tokens = re.split(r'([?!]|\\s)', text)\n",
        "  tokens_stripped = [token for token in tokens if token.strip()]\n",
        "  return tokens_stripped\n",
        "\n",
        "def tokenise_text_no_punctuation(text):\n",
        "  temp_tokens = tokenise_text_naive(text)\n",
        "  tokens = [token for token in temp_tokens if re.sub(r'[^\\w]', '', token)]\n",
        "  return tokens\n",
        "\n",
        "tokenised_sentence = tokenise_text_naive(sentence)\n",
        "tokenised_sentence_2 = tokenise_text_no_punctuation(sentence)\n",
        "print(tokenised_sentence)\n",
        "print(tokenised_sentence_2)\n"
      ],
      "metadata": {
        "id": "_H25LV7nlwBC",
        "outputId": "f13b45cf-308f-42b6-8b8b-da81f3896641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_H25LV7nlwBC",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'joy', 'of', 'learning', 'machine', 'learning', '?', '!']\n",
            "['The', 'joy', 'of', 'learning', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit tests\n",
        "\n",
        "import unittest\n",
        "\n",
        "class TestTokeniser(unittest.TestCase):\n",
        "\n",
        "  def test_naive_space_toknisation(self):\n",
        "    sentence = \"The joy of learning machine learning!\"\n",
        "    self.assertEqual(tokenise_text_naive(sentence), [\"The\", \"joy\", \"of\", \"learning\", \"machine\", \"learning\", \"!\"])\n",
        "\n",
        "  def test_tokenise_text_no_punctuation(self):\n",
        "    sentence = \"The joy of learning machine learning!\"\n",
        "    self.assertEqual(tokenise_text_no_punctuation(sentence), [\"The\", \"joy\", \"of\", \"learning\", \"machine\", \"learning\"])\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  unittest.main(argv=[''], exit=False)"
      ],
      "metadata": {
        "id": "zuxgIeutnPuS",
        "outputId": "30ce6d36-56bc-4da3-d1af-edde92a6d3b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zuxgIeutnPuS",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.004s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokeniserV1():\n",
        "  def tokenise_text_naive(self, text):\n",
        "    \"\"\"\n",
        "    Takes a string as input and returns a list of tokens\n",
        "    \"\"\"\n",
        "    tokens = re.split(r'([?!]|\\s)', text)\n",
        "    tokens_stripped = [token for token in tokens if token.strip()]\n",
        "    return tokens_stripped\n",
        "\n",
        "  def tokenise_text_no_punctuation(self, text):\n",
        "    temp_tokens = self.tokenise_text_naive(text)\n",
        "    tokens = [token for token in temp_tokens if re.sub(r'[^\\w]', '', token)]\n",
        "    return tokens\n",
        "\n",
        "  def create_vocabulary(self, text):\n",
        "    tokens = sorted(tokenise_text_no_punctuation(text))\n",
        "    tokens.append(\"<|endofsequence|>\")\n",
        "    unique_tokens = set(tokens)\n",
        "    self.vocab = { token:integer for integer,token in enumerate(unique_tokens) }\n",
        "    self.id_to_token = { integer:token for integer,token in enumerate(unique_tokens) }\n",
        "    return self.vocab\n",
        "\n",
        "\n",
        "sentence = \"The joy of learning machine learning?!\"\n",
        "tokeniser = TokeniserV1()\n",
        "print(tokeniser.tokenise_text_naive(sentence))\n",
        "print(tokeniser.tokenise_text_no_punctuation(sentence))\n",
        "print(tokeniser.create_vocabulary(sentence))\n",
        "\n",
        "print(tokeniser.vocab)\n"
      ],
      "metadata": {
        "id": "GkvJZodG0mFY",
        "outputId": "ed1e7439-3304-4abc-f3e3-bcc7f97b61fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GkvJZodG0mFY",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'joy', 'of', 'learning', 'machine', 'learning', '?', '!']\n",
            "['The', 'joy', 'of', 'learning', 'machine', 'learning']\n",
            "{'joy': 0, 'The': 1, 'learning': 2, 'of': 3, 'machine': 4, '<|endofsequence|>': 5}\n",
            "{'joy': 0, 'The': 1, 'learning': 2, 'of': 3, 'machine': 4, '<|endofsequence|>': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTokeniserV1(unittest.TestCase):\n",
        "\n",
        "  def test_vocabulary_should_not_have_repeated_entrie(self):\n",
        "    sentence = \"The joy of learning machine learning?!\"\n",
        "    tokeniser = TokeniserV1()\n",
        "    tokeniser.create_vocabulary(sentence)\n",
        "    print(tokeniser.vocab)\n",
        "    self.assertEqual(len(tokeniser.vocab), 5+1)\n",
        "\n",
        "  def test_vocabulary_should_end_with_EOS_token(self):\n",
        "    sentence = \"The joy of learning machine learning?!\"\n",
        "    tokeniser = TokeniserV1()\n",
        "    tokeniser.create_vocabulary(sentence)\n",
        "    last_vocab_token = tokeniser.vocab.get(len(tokeniser.vocab) - 1)\n",
        "    print(last_vocab_token)\n",
        "    self.assertEqual(last_vocab_token, \"<|endofsequence|>\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  unittest.main(argv=[''], exit=False, verbosity=2)"
      ],
      "metadata": {
        "id": "mxu_44i0oXw8",
        "outputId": "6aa8fe54-05cb-40a1-9022-506760f8392a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mxu_44i0oXw8",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_naive_space_toknisation (__main__.TestTokeniser.test_naive_space_toknisation) ... ok\n",
            "test_tokenise_text_no_punctuation (__main__.TestTokeniser.test_tokenise_text_no_punctuation) ... ok\n",
            "test_vocabulary_should_end_with_EOS_token (__main__.TestTokeniserV1.test_vocabulary_should_end_with_EOS_token) ... FAIL\n",
            "test_vocabulary_should_not_have_repeated_entrie (__main__.TestTokeniserV1.test_vocabulary_should_not_have_repeated_entrie) ... ok\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_vocabulary_should_end_with_EOS_token (__main__.TestTokeniserV1.test_vocabulary_should_end_with_EOS_token)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-104-0ef04f5b5ea5>\", line 16, in test_vocabulary_should_end_with_EOS_token\n",
            "    self.assertEqual(last_vocab_token, \"<|endofsequence|>\")\n",
            "AssertionError: None != '<|endofsequence|>'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.009s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "{'joy': 0, 'The': 1, 'learning': 2, 'of': 3, 'machine': 4, '<|endofsequence|>': 5}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}