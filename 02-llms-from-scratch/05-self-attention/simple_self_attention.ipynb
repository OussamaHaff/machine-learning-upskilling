{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OussamaHaff/machine-learning-upskilling/blob/main/02-llms-from-scratch/05-self-attention/simple_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Self Attention Without Weights"
      ],
      "metadata": {
        "id": "initial_id"
      },
      "id": "initial_id"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Embeddings"
      ],
      "metadata": {
        "id": "Hg8ig_zgSp1T"
      },
      "id": "Hg8ig_zgSp1T"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sentence: \"Your journey starts with one step\"\n",
        "inputs = torch.tensor([\n",
        "    [0.43, 0.15, 0.89], # Your\n",
        "    [0.55, 0.87, 0.66], # journey\n",
        "    [0.57, 0.85, 0.64], # starts\n",
        "    [0.22, 0.58, 0.33], # with\n",
        "    [0.77, 0.25, 0.10], # one\n",
        "    [0.05, 0.80, 0.55], # step\n",
        "])\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "T5z47aF7QcJE",
        "outputId": "111fb936-7d04-4e84-eefa-38cdeeeed0bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "T5z47aF7QcJE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4300, 0.1500, 0.8900],\n",
            "        [0.5500, 0.8700, 0.6600],\n",
            "        [0.5700, 0.8500, 0.6400],\n",
            "        [0.2200, 0.5800, 0.3300],\n",
            "        [0.7700, 0.2500, 0.1000],\n",
            "        [0.0500, 0.8000, 0.5500]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Attention Scores\n",
        "\n",
        "# This is formatted as code\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d8PjHsg8SmOr"
      },
      "id": "d8PjHsg8SmOr"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "\n",
        "# Attention weight for \"journey\"\n",
        "journey_embedding_query_token = inputs[1]\n",
        "print(f\"Query token embeddings:\\n\", journey_embedding_query_token)\n",
        "\n",
        "journey_attention_scores = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  journey_attention_scores[i] = torch.dot(x_i, journey_embedding_query_token)\n",
        "\n",
        "print(f\"Self Attention scores for query:\\n\",journey_attention_scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "gGYW7-AcTK3Y",
        "outputId": "7b71fd18-52dd-4fa4-c962-3cebb57e1134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gGYW7-AcTK3Y",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query token embeddings:\n",
            " tensor([0.5500, 0.8700, 0.6600])\n",
            "Self Attention scores for query:\n",
            " tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Scores Normalisation\n",
        "\n",
        "### Basic Normalisation"
      ],
      "metadata": {
        "id": "-6gsOu4DWRB3"
      },
      "id": "-6gsOu4DWRB3"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Attentions Scores W_2T\n",
        "#       0.9544             1.4950                 1.4754             0.8434             0.7070              1.0865\n",
        "\n",
        "journey_attention_weights = journey_attention_scores / journey_attention_scores.sum()\n",
        "print(f\"Normalised self attention weights:\\n\", journey_attention_weights)\n",
        "print(f\"Normalised self attention weights sum:\\n\", journey_attention_weights.sum())"
      ],
      "metadata": {
        "id": "UXWFGQZ-WWjB",
        "outputId": "a535cec5-8d9d-4675-d236-e8ef8ec0f398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UXWFGQZ-WWjB",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalised self attention weights:\n",
            " tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Normalised self attention weights sum:\n",
            " tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Normalisation - Naive"
      ],
      "metadata": {
        "id": "7ouMvAhTYBeb"
      },
      "id": "7ouMvAhTYBeb"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Attentions Scores W_2T\n",
        "#       0.9544             1.4950                 1.4754             0.8434             0.7070              1.0865\n",
        "#\n",
        "# Normalised Wights\n",
        "#       0.1455             0.2278                  0.2249            0.1285             0.1077              0.1656\n",
        "\n",
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "journey_attention_weights_sfmx_nv = softmax_naive(journey_attention_scores)\n",
        "\n",
        "print(\"Softmax Normalisation - Naive\")\n",
        "print(f\"Normalised self attention weights:\\n\", journey_attention_weights_sfmx_nv)\n",
        "print(f\"Normalised self attention weights sum:\\n\", journey_attention_weights_sfmx_nv.sum())"
      ],
      "metadata": {
        "id": "CaLncVoEYF6e",
        "outputId": "72011ddc-0158-4010-e118-3abe264d03f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CaLncVoEYF6e",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Normalisation - Naive\n",
            "Normalised self attention weights:\n",
            " tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Normalised self attention weights sum:\n",
            " tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Normalisation - PyTorch (best)"
      ],
      "metadata": {
        "id": "i2EekOMkfj2H"
      },
      "id": "i2EekOMkfj2H"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Attentions Scores W_2T\n",
        "#       0.9544             1.4950                 1.4754             0.8434             0.7070              1.0865\n",
        "#\n",
        "# Normalised Wights\n",
        "#       0.1455             0.2278                  0.2249            0.1285             0.1077             0.1656\n",
        "#\n",
        "# Normalised Wights - Softmax Naive\n",
        "#       0.1385             0.2379                  0.2333            0.1240             0.1082            0.1581\n",
        "\n",
        "\n",
        "journey_attention_weights_sfmx = torch.softmax(journey_attention_scores, dim=0)\n",
        "print(\"Softmax Normalisation - PyTorch\")\n",
        "print(f\"Normalised self attention weights:\\n\", journey_attention_weights_sfmx)\n",
        "print(f\"Normalised self attention weights sum:\\n\", journey_attention_weights_sfmx.sum())"
      ],
      "metadata": {
        "id": "27KrJWzIfpgf",
        "outputId": "0f3626c9-5ee5-445b-d64b-728663ccea40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "27KrJWzIfpgf",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Normalisation - PyTorch\n",
            "Normalised self attention weights:\n",
            " tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Normalised self attention weights sum:\n",
            " tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Context Vector"
      ],
      "metadata": {
        "id": "hdMGa4KPhEQk"
      },
      "id": "hdMGa4KPhEQk"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Normalised Wights - Softmax PyTorch\n",
        "#       0.1385              0.2379                  0.2333            0.1240            0.1082            0.1581\n",
        "\n",
        "\n",
        "# Attention weight for \"journey\"\n",
        "journey_embedding_query_token = inputs[1]\n",
        "context_vector = torch.zeros(journey_embedding_query_token.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  context_vector += journey_attention_weights_sfmx[i]*x_i\n",
        "\n",
        "print(context_vector)"
      ],
      "metadata": {
        "id": "CRYJd_0WhId2",
        "outputId": "df383f94-c655-485e-dc49-427b5c3501fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CRYJd_0WhId2",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Context Vector of the full input"
      ],
      "metadata": {
        "id": "J6AJd6w-cB8B"
      },
      "id": "J6AJd6w-cB8B"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your                journey            starts              with                 one                  step\n",
        "\n",
        "#\n",
        "# inputs = torch.tensor([\n",
        "#    [0.43, 0.15, 0.89], # Your\n",
        "#    [0.55, 0.87, 0.66], # journey\n",
        "#    [0.57, 0.85, 0.64], # starts\n",
        "#    [0.22, 0.58, 0.33], # with\n",
        "#    [0.77, 0.25, 0.10], # one\n",
        "#    [0.05, 0.80, 0.55], # step\n",
        "# ])\n",
        "\n",
        "\n",
        "# For each token in the sentence we have 6 scores - sentence has 6 tokens\n",
        "sentence_attention_scores = torch.zeros(6, 6)\n",
        "\n",
        "print(f\"sentence_attention_scores \\n\", sentence_attention_scores)\n",
        "print(f\"inputs \\n\", inputs)\n",
        "\n",
        "for token_index, token_embeddings in enumerate(inputs):\n",
        "  for context_index, context_embedding in enumerate(inputs):\n",
        "    sentence_attention_scores[token_index, context_index] = torch.dot(token_embeddings, context_embedding)\n",
        "\n",
        "\n",
        "print(f\"sentence_attention_scores \\n\", sentence_attention_scores)\n"
      ],
      "metadata": {
        "id": "qcjbEymQfTCR",
        "outputId": "2ea79f6a-42aa-48fa-e506-5f5e97b4c967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qcjbEymQfTCR",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_attention_scores \n",
            " tensor([[0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "inputs \n",
            " tensor([[0.4300, 0.1500, 0.8900],\n",
            "        [0.5500, 0.8700, 0.6600],\n",
            "        [0.5700, 0.8500, 0.6400],\n",
            "        [0.2200, 0.5800, 0.3300],\n",
            "        [0.7700, 0.2500, 0.1000],\n",
            "        [0.0500, 0.8000, 0.5500]])\n",
            "sentence_attention_scores \n",
            " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The same could be acheived via matrix multiplication\n",
        "sentence_attention_scores_matrix = inputs @ inputs.T\n",
        "\n",
        "print(f\"sentence_attention_scores_matrix \\n\", sentence_attention_scores_matrix)"
      ],
      "metadata": {
        "id": "k30GrjyAkQbX",
        "outputId": "46ac40bb-39c4-4fc2-dc37-4603c8ecebcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "k30GrjyAkQbX",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_attention_scores_matrix \n",
            " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax to calculate weights from scors\n",
        "# dim=-1 to apply the softmax based on the last dimention of the matrix - in this\n",
        "# case it's a 2D matric, so it will apply on columns.\n",
        "sentence_attention_weights = torch.softmax(sentence_attention_scores_matrix, dim=-1)\n",
        "\n",
        "print(f\"sentence_attention_weights \\n\", sentence_attention_weights)"
      ],
      "metadata": {
        "id": "zshL41-Flusg",
        "outputId": "945d6e2f-ea22-4a0d-c3bc-bceb1b0dbf65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zshL41-Flusg",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_attention_weights \n",
            " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Context Vectors\n",
        "sentence_context_vectors = sentence_attention_weights @ inputs\n",
        "\n",
        "print(f\"sentence_context_vectors \\n\", sentence_context_vectors)"
      ],
      "metadata": {
        "id": "Vw2FLdtooGkG",
        "outputId": "305e83e3-a96a-41dc-8360-ac2cd62820c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Vw2FLdtooGkG",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_context_vectors \n",
            " tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}