{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OussamaHaff/machine-learning-upskilling/blob/main/02-llms-from-scratch/05-self-attention/simple_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Self Attention Without Weights"
      ],
      "metadata": {
        "id": "initial_id"
      },
      "id": "initial_id"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Embeddings"
      ],
      "metadata": {
        "id": "Hg8ig_zgSp1T"
      },
      "id": "Hg8ig_zgSp1T"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sentence: \"Your journey starts with one step\"\n",
        "inputs = torch.tensor([\n",
        "    [0.43, 0.15, 0.89], # Your\n",
        "    [0.55, 0.87, 0.66], # journey\n",
        "    [0.57, 0.85, 0.64], # starts\n",
        "    [0.22, 0.58, 0.33], # with\n",
        "    [0.77, 0.25, 0.10], # one\n",
        "    [0.05, 0.80, 0.55], # step\n",
        "])\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "T5z47aF7QcJE",
        "outputId": "0ef2ce0f-24db-479f-8139-ddad9452098b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "T5z47aF7QcJE",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4300, 0.1500, 0.8900],\n",
            "        [0.5500, 0.8700, 0.6600],\n",
            "        [0.5700, 0.8500, 0.6400],\n",
            "        [0.2200, 0.5800, 0.3300],\n",
            "        [0.7700, 0.2500, 0.1000],\n",
            "        [0.0500, 0.8000, 0.5500]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Attention Scores\n",
        "\n",
        "# This is formatted as code\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d8PjHsg8SmOr"
      },
      "id": "d8PjHsg8SmOr"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "\n",
        "# Attention weight for \"journey\"\n",
        "journey_embedding_query_token = inputs[1]\n",
        "print(f\"Query token embeddings:\\n\", journey_embedding_query_token)\n",
        "\n",
        "journey_attention_scores = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  journey_attention_scores[i] = torch.dot(x_i, journey_embedding_query_token)\n",
        "\n",
        "print(f\"Self Attention scores for query:\\n\",journey_attention_scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "gGYW7-AcTK3Y",
        "outputId": "013eb41b-da4b-4282-bf8e-5e7791c8da68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gGYW7-AcTK3Y",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query token embeddings:\n",
            " tensor([0.5500, 0.8700, 0.6600])\n",
            "Self Attention scores for query:\n",
            " tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Scores Normalisation\n",
        "\n",
        "### Basic Normalisation"
      ],
      "metadata": {
        "id": "-6gsOu4DWRB3"
      },
      "id": "-6gsOu4DWRB3"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Attentions Scores W_2T\n",
        "#       0.9544             1.4950                 1.4754             0.8434             0.7070              1.0865\n",
        "\n",
        "journey_attention_weights = journey_attention_scores / journey_attention_scores.sum()\n",
        "print(f\"Normalised self attention weights:\\n\", journey_attention_weights)\n",
        "print(f\"Normalised self attention weights sum:\\n\", journey_attention_weights.sum())"
      ],
      "metadata": {
        "id": "UXWFGQZ-WWjB",
        "outputId": "2378fcb3-0c8b-4041-ee77-77fce89e360c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UXWFGQZ-WWjB",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalised self attention weights:\n",
            " tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Normalised self attention weights sum:\n",
            " tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Normalisation - Naive"
      ],
      "metadata": {
        "id": "7ouMvAhTYBeb"
      },
      "id": "7ouMvAhTYBeb"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Attentions Scores W_2T\n",
        "#       0.9544             1.4950                 1.4754             0.8434             0.7070              1.0865\n",
        "#\n",
        "# Normalised Wights\n",
        "#       0.1455             0.2278                  0.2249            0.1285             0.1077              0.1656\n",
        "\n",
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "journey_attention_weights_sfmx_nv = softmax_naive(journey_attention_scores)\n",
        "\n",
        "print(\"Softmax Normalisation - Naive\")\n",
        "print(f\"Normalised self attention weights:\\n\", journey_attention_weights_sfmx_nv)\n",
        "print(f\"Normalised self attention weights sum:\\n\", journey_attention_weights_sfmx_nv.sum())"
      ],
      "metadata": {
        "id": "CaLncVoEYF6e",
        "outputId": "024cf336-8c31-4987-9945-ae298556d48f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CaLncVoEYF6e",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Normalisation - Naive\n",
            "Normalised self attention weights:\n",
            " tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Normalised self attention weights sum:\n",
            " tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Normalisation - PyTorch (best)"
      ],
      "metadata": {
        "id": "i2EekOMkfj2H"
      },
      "id": "i2EekOMkfj2H"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Attentions Scores W_2T\n",
        "#       0.9544             1.4950                 1.4754             0.8434             0.7070              1.0865\n",
        "#\n",
        "# Normalised Wights\n",
        "#       0.1455             0.2278                  0.2249            0.1285             0.1077             0.1656\n",
        "#\n",
        "# Normalised Wights - Softmax Naive\n",
        "#       0.1385             0.2379                  0.2333            0.1240             0.1082            0.1581\n",
        "\n",
        "\n",
        "journey_attention_weights_sfmx = torch.softmax(journey_attention_scores, dim=0)\n",
        "print(\"Softmax Normalisation - PyTorch\")\n",
        "print(f\"Normalised self attention weights:\\n\", journey_attention_weights_sfmx)\n",
        "print(f\"Normalised self attention weights sum:\\n\", journey_attention_weights_sfmx.sum())"
      ],
      "metadata": {
        "id": "27KrJWzIfpgf",
        "outputId": "35a44bca-b472-459d-ee77-c75fa2e1d13d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "27KrJWzIfpgf",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Normalisation - PyTorch\n",
            "Normalised self attention weights:\n",
            " tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Normalised self attention weights sum:\n",
            " tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Context Vector"
      ],
      "metadata": {
        "id": "hdMGa4KPhEQk"
      },
      "id": "hdMGa4KPhEQk"
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.43, 0.15, 0.89]  [0.55, 0.87, 0.66]  [0.57, 0.85, 0.64]  [0.22, 0.58, 0.33]  [0.77, 0.25, 0.10]  [0.05, 0.80, 0.55]\n",
        "#       Your           >>> journey <<<            starts              with               one              step\n",
        "#                           query\n",
        "#\n",
        "# Normalised Wights - Softmax PyTorch\n",
        "#       0.1385              0.2379                  0.2333            0.1240            0.1082            0.1581\n",
        "\n",
        "\n",
        "# Attention weight for \"journey\"\n",
        "journey_embedding_query_token = inputs[1]\n",
        "context_vector = torch.zeros(journey_embedding_query_token.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  context_vector += journey_attention_weights_sfmx[i]*x_i\n",
        "\n",
        "print(context_vector)"
      ],
      "metadata": {
        "id": "CRYJd_0WhId2",
        "outputId": "aa98fef6-75ca-4d65-dcae-9f1dd3405a91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CRYJd_0WhId2",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}